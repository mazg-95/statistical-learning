{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05fa1ab8-3ab6-4b5f-9648-c98c6ca2c418",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95177cf8-7a6b-4aea-ab6a-c48310769c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn import svm\n",
    "from datetime import datetime\n",
    "from scipy.stats import mode\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebf811a-c1a2-4448-a74e-4c7764966b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "  import tensorflow.compat.v1 as tf\n",
    "  tf.compat.v1.disable_v2_behavior()\n",
    "  tf.compat.v1.disable_eager_execution()\n",
    "  print(\"Enabled compatitility to tf1.x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399b55f8-5fac-4314-8ed8-7f3cf8b1d2f3",
   "metadata": {},
   "source": [
    "SVM viene de Support Vector Machines, es un algoritmo diferente a la regression logistica dado, que aunque se basa en las mismas ideas.\n",
    "Agrega una seccion de confianza que no tiene la regresion logistica. Y eso muchas veces lo hace mas robusto y capaz de generalizar mejor el aprendizaje.\n",
    "\n",
    "### SVM \n",
    "\n",
    "Es un algoritmo de aprendizaje supervisado creado por Vladimir Vapnik junto a otros colegas en el AT&T Bell Laboratories. Y es un algoritmo que es ampliamente popular en aplicaciones biologicas.\n",
    "Es usado para clasificacion, regresion o detectar outliers. \n",
    "\n",
    "\n",
    "#### Ventajas\n",
    "- Efectivo en espcaios de muchas dimensiones\n",
    "- Se mantiene efectivo en casos donde el numero de dimensiones es mayor al numero de muestras.\n",
    "- Usa un subset de puntos de entrenamiento en la funcion de decision que utiliza de soporte. (Support Vectors). \n",
    "- Versatil, ya que se puede customizar el kernel para la funcion de decision.\n",
    "\n",
    "\n",
    "#### Desventajas\n",
    "- Es posible el sobreajuste si el numero de caracteristicas es mucho mayor que el numero de muestras. Dependera de la decision de los hyperparametros para evitarlo.\n",
    "- No calcula estimaciones de probabildad directamente, sino calcula un 5 Fold CV costoso.\n",
    "\n",
    "\n",
    "Ejemplo de lineas de decision en regresion logistica:\n",
    "<figure>\n",
    "<img id=\"img1\" src=\"https://miro.medium.com/max/600/0*9jEWNXTAao7phK-5.png\">\n",
    "</figure>\n",
    "\n",
    "Ejemplo de una linea de decision optima (SVM):\n",
    "<figure>\n",
    "<img id=\"img1\" src=\"https://miro.medium.com/max/600/0*0o8xIA4k3gXUDCFU.png\">\n",
    "</figure>\n",
    "\n",
    "\n",
    "Los modelos SVM se consideran algoritmos de aprendizaje robustos dado ha estas lineas de soporte que ayudan a encontrar una barrera de decision optima. El plano de separacion entre clases que genera, le otorga la robustez que le caracteriza.\n",
    "\n",
    "SVM  se basa en que la predicion ya no tomara valores de 0 o 1. Sino cambia estos valores por -1 y 1. Ademas la funcion de costo tambien cambia dado en parte a este cambio. Pero la hipotesis por lo tanto tomara la forma siguiente: \n",
    "$$h_{w,b}(x) = g(w^{T} x + b)$$\n",
    "\n",
    "Notese que sigue siendo una hipotesis similar a la regresion logisitca. La diferencia ahora es que se ha aumentado el margen para separar las clases. Lo que se busca ahora es que *g(z) = 1* cuando *z* que es igual a  $w^{T} x + b)$ sea mucho mayor a 1. La frontera de decision ya no es solamente una linea. Por lo tanto *g(z) = 1* cuando *z* sea negativo. \n",
    "\n",
    "Nuestra funcion de costo entoces buscara generar mayor penalizacion para los casos en los que si $y = 1$ y $w^{T} x + b) < 0$ y cuando  $y = 0$ pero $w^{T} x + b) > 0$\n",
    "\n",
    "### Funcion de costo\n",
    "La funcion de costo es parecida en forma a la regresio logistica, pero ahora la funcion de SVM tiene forma de bisagra. Es por eso que le dicen hinge. \n",
    "Ahora para SVM se utilizara la siguiente: \n",
    "\n",
    "$$  C * \\sum_{i=1}^{n}\\max(0,1-y_i(w^{T}x_i + b)) + \\frac{1}{2}\\left\\|w\\right\\|^2$$\n",
    "\n",
    "Tambien se utiliza un parametro diferente para la regularizacion. En regresion logistica normalmente se aplicaba a los parametros segun fuera el metodo (L2 o L1), y es el hyperparametro denotado por lambda. \n",
    "\n",
    "$$A + \\lambda * B$$\n",
    "\n",
    "En SVM se utilziara el hyperparametro `C`:\n",
    "$$ C * A + B $$\n",
    "Es una forma de manejar el trade off del sesgo y varianza tambien.\n",
    "\n",
    "### Kernel Tricks\n",
    "La funcion Kernel de el algoritmo de SVM es una funcion que nos permite transformar los datos de entrada (caracteristicas) moviendolo a otro espacio y encontrando hyperplanos que separen de mejor manera los datos. La idea es encontrar una funcion que permita comparar que tan similares son dos vectores. Si dos vectores son similares el producto punto entre los vectores se hara mas grande, lo que precisamente buscamos por lo tanto es una funcion que separe nuestras observaciones al tomar un valor grande. \n",
    "\n",
    "SVM se dice ser flexible gracias a la capacidad de encontrar mejores barreras de decision gracias a que se puede customizar por medio de la funcion kernel. \n",
    "Existen Kernels ya definidos como los que ya trae la libreria de Scikit Learn: \n",
    "\n",
    "- linear: $\\langle x, x'\\rangle.$\n",
    "- polynomial: $(\\gamma \\langle x, x'\\rangle + r)^d$\n",
    "- rbf: $\\exp(-\\gamma \\|x-x'\\|^2)$\n",
    "- sigmoid $\\tanh(\\gamma \\langle x,x'\\rangle + r)$\n",
    "\n",
    "Pero la funcion de Kernel permite customizar el algoritmo SVM, y esto hace posible encontrar nuevos planos que separen de mejor manera los datos.\n",
    "\n",
    "### Conclusiones\n",
    "- A diferencia de otros algoritmos de clasificacion que tienen como salida una probabilidad, Support Vector Machine no tiene de salida una probabilidad. Si no tiene como resultado la clase directamente. \n",
    "- SVM tuvo una gran aceptacion e interes en aplicaciones medicas antes de la tendencia actual de Deep Learning, y la razon es su buen margen de confianza aun aplicado a espacios de dimensiones mayores.\n",
    "- Sino se necesita una probabilidad o nivel de certeza, es una buena opcion para aplicar en cualquier tipo de deataset gracias a su flexibilidad con los kernel tricks\n",
    "\n",
    "\n",
    "### Bibliografia\n",
    "[Scikit Learn SVM](https://scikit-learn.org/stable/modules/svm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6d2bf-62d1-40ad-b33f-1af0979b9eb6",
   "metadata": {},
   "source": [
    "### Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ded5fe-bb2b-46fb-9303-a564f5da2d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
